{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import accimage\n",
    "except ImportError:\n",
    "    accimage = None\n",
    "    \n",
    "\n",
    "def _is_pil_image(img):\n",
    "    if accimage is not None:\n",
    "        return isinstance(img, (Image.Image, accimage.Image))\n",
    "    else:\n",
    "        return isinstance(img, Image.Image)\n",
    "    \n",
    "    \n",
    "def _rgb2hsv(testing):\n",
    "    max_vals, _ = torch.max(testing, dim=0)\n",
    "    min_vals, _ = torch.min(testing, dim=0)\n",
    "    v = max_vals\n",
    "    s = (max_vals - min_vals) / max_vals\n",
    "    df = max_vals - min_vals\n",
    "    r, g, b = testing[0], testing[1], testing[2]\n",
    "    \n",
    "    \n",
    "    h = max_vals != min_vals\n",
    "    hr =  (max_vals == r) * ((g - b) / df) \n",
    "    hg =  (max_vals == g) * ((b - r) / df + 2.0)\n",
    "    hb =  (max_vals == b) * ((r - g) / df + 4.0)    \n",
    "    h = h * (hr + hg + hb)\n",
    "    h = (h / 6.0) % 1.0\n",
    "    h[h != h] = 0\n",
    "    return torch.stack((h, s, v))\n",
    " \n",
    "def _hsv2rgb(testing):\n",
    "    h, s, v = testing[0], testing[1], testing[2]\n",
    "    i = (h * 6.0).type(torch.IntTensor)\n",
    "    f = (h * 6.0) - i\n",
    "    p = v * (1.0 - s)\n",
    "    q = v * (1.0 - f * s)\n",
    "    t = v * (1.0 - (1 - f) * s)\n",
    "    i = i % 6\n",
    "    r = (i == 0) * v + (i == 1) * q +  (i == 2) * p + (i == 3) * p + (i == 4) * t + (i == 5) * v \n",
    "    g = (i == 0) * t + (i == 1) * v +  (i == 2) * v + (i == 3) * q + (i == 4) * p + (i == 5) * p\n",
    "    b = (i == 0) * p + (i == 1) * p +  (i == 2) * t + (i == 3) * v + (i == 4) * v + (i == 5) * q\n",
    "\n",
    "    return torch.stack((r, g, b))\n",
    "    \n",
    "def adjust_hue2(img, hue_factor):\n",
    "    \"\"\"Adjust hue of an image.\n",
    "\n",
    "    The image hue is adjusted by converting the image to HSV and\n",
    "    cyclically shifting the intensities in the hue channel (H).\n",
    "    The image is then converted back to original image mode.\n",
    "\n",
    "    `hue_factor` is the amount of shift in H channel and must be in the\n",
    "    interval `[-0.5, 0.5]`.\n",
    "\n",
    "    See `Hue`_ for more details.\n",
    "\n",
    "    .. _Hue: https://en.wikipedia.org/wiki/Hue\n",
    "\n",
    "    Args:\n",
    "        img (PIL Image or torch.Tensor): Image to be adjusted.\n",
    "                                         Input can be PIL or torch.Tensor.\n",
    "        hue_factor (float):  How much to shift the hue channel. Should be in\n",
    "            [-0.5, 0.5]. 0.5 and -0.5 give complete reversal of hue channel in\n",
    "            HSV space in positive and negative direction respectively.\n",
    "            0 means no shift. Therefore, both -0.5 and 0.5 will give an image\n",
    "            with complementary colors while 0 gives the original image.\n",
    "\n",
    "    Returns:\n",
    "        PIL Image or torch.Tensor: Hue adjusted image.\n",
    "        If input is PIL Image, return PIL Image\n",
    "        If input is torch.Tensor, return torch.Tensor\n",
    "    \"\"\"\n",
    "    if not(-0.5 <= hue_factor <= 0.5):\n",
    "        raise ValueError('hue_factor is not in [-0.5, 0.5].'.format(hue_factor))\n",
    "    \n",
    "    if not _is_pil_image(img) and type(img) is not torch.Tensor:\n",
    "        raise TypeError('img should be PIL Image or torch.Tensor. Got {}'.format(type(img)))\n",
    "        \n",
    "    if _is_pil_image(img):\n",
    "\n",
    "        input_mode = img.mode\n",
    "        if input_mode in {'L', '1', 'I', 'F'}:\n",
    "            return img\n",
    "\n",
    "        h, s, v = img.convert('HSV').split()\n",
    "#         print(h, s, v)\n",
    "#         print(transform(h))\n",
    "        np_h = np.array(h, dtype=np.uint8)\n",
    "        # uint8 addition take cares of rotation across boundaries\n",
    "        with np.errstate(over='ignore'):\n",
    "            np_h += np.uint8(hue_factor * 255)\n",
    "        h = Image.fromarray(np_h, 'L')\n",
    "        img = Image.merge('HSV', (h, s, v)).convert(input_mode)\n",
    "        return img\n",
    "    else:\n",
    "        assert type(img) is torch.Tensor\n",
    "        assert len(img.shape) == 3 # input img must be 3D torch.Tensor\n",
    "        assert img.shape[0] == 3 # input img must have 3 channels \n",
    "        # the default ToTensor in torchvision scale the RGB from [0,255] to [0, 1]\n",
    "        img = _rgb2hsv(img)\n",
    "        h, s, v = img[0], img[1], img[2]\n",
    "        new_h = h * 255\n",
    "        new_h = new_h.type(torch.IntTensor)\n",
    "        new_h += int(hue_factor * 255)\n",
    "        new_h = new_h.type(torch.FloatTensor)\n",
    "        new_h = new_h / 255.0\n",
    "        new_img = _hsv2rgb(torch.stack((new_h, s, v)))\n",
    "        return new_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open a new image\n",
    "img_cat = Image.open(\"/Users/xni/Documents/pytorch_task/cat.jpg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the old adjust_hue \n",
    "# when input is PIL image, we use the old adjust_hue\n",
    "img_new = adjust_hue2(img_cat, 0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the image adjusted using old method\n",
    "img_new.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 559, 838])"
      ]
     },
     "execution_count": 560,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform the image into torch.Tensor\n",
    "transform = transforms.ToTensor()\n",
    "img_cat_tensor = transform(img_cat)\n",
    "img_cat_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the new adjust_hue\n",
    "# when input is torch.Tensor, we use the new adjust_hue\n",
    "new_cat = adjust_hue2(img_cat_tensor, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the image back into PIL image\n",
    "transform2 = transforms.ToPILImage()\n",
    "img_cal_pil = transform2(new_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the image adjusted using new method\n",
    "\n",
    "img_cal_pil.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
